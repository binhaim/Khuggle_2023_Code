{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "s3WlGzKkzJ6A",
        "vD-o5QWyKN52",
        "lIvTIH0aKVdH",
        "VrmzbB1e7ekp",
        "omQbrznAK-vP",
        "JCp9gmxwg_E6"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Base Setting"
      ],
      "metadata": {
        "id": "ZXO5JihDGULU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2eT0mYjcxCL",
        "outputId": "df5e1337-9698-4e3f-89a1-b0935d183afa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ninja tqdm lpips ftfy regex -q\n",
        "!pip install --upgrade gdown -q\n",
        "!pip install wandb -q\n",
        "!pip install torchvision==0.13.0 -q\n",
        "!pip install git+https://github.com/openai/CLIP.git -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1cLKmujGZzx",
        "outputId": "0df5cf45-3cae-4300-8340-d70424f3ef1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/307.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/307.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.12.0 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.12.0 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rinongal/StyleGAN-nada.git #Get StyleGAN-nada"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jQm1Nt0G0sA",
        "outputId": "ce068a4e-8ada-4d61-ca39-145feb0ef601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'StyleGAN-nada'...\n",
            "remote: Enumerating objects: 380, done.\u001b[K\n",
            "remote: Counting objects: 100% (125/125), done.\u001b[K\n",
            "remote: Compressing objects: 100% (81/81), done.\u001b[K\n",
            "remote: Total 380 (delta 62), reused 78 (delta 41), pack-reused 255\u001b[K\n",
            "Receiving objects: 100% (380/380), 23.54 MiB | 37.20 MiB/s, done.\n",
            "Resolving deltas: 100% (111/111), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os #Get Baseline\n",
        "\n",
        "url = \"https://drive.google.com/uc?id=1RXb_yQjQMV9Nu36SYLEUl6sCTiaSg7hu\"\n",
        "\n",
        "before_files = set(os.listdir())\n",
        "\n",
        "!gdown {url}\n",
        "\n",
        "after_files = set(os.listdir())\n",
        "\n",
        "downloaded_files = after_files - before_files\n",
        "\n",
        "if downloaded_files:\n",
        "    filename = downloaded_files.pop()\n",
        "    print(f\"Downloaded file: {filename}\")\n",
        "    downloaded_filepath = filename\n",
        "else:\n",
        "    print(\"No file downloaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suLF-mdhGcRq",
        "outputId": "6014d089-dc18-4c06-fc7e-60b005fdabfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (uriginal): https://drive.google.com/uc?id=1RXb_yQjQMV9Nu36SYLEUl6sCTiaSg7hu\n",
            "From (redirected): https://drive.google.com/uc?id=1RXb_yQjQMV9Nu36SYLEUl6sCTiaSg7hu&confirm=t&uuid=8b6a42ae-66cf-46c4-9da7-ad02a067266b\n",
            "To: /content/khuggle_baseline_revised.zip\n",
            "100% 691M/691M [00:11<00:00, 59.4MB/s]\n",
            "Downloaded file: khuggle_baseline_revised.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q {downloaded_filepath}"
      ],
      "metadata": {
        "id": "YaJfSK17GgYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/StyleGAN-nada/ZSSGAN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QaK7eotKcrv",
        "outputId": "8c5c5f90-5ffd-4e30-8a44-9be01ddcc122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/StyleGAN-nada/ZSSGAN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Covert Domain of Generator using StyleGan-NADA"
      ],
      "metadata": {
        "id": "dutuwOcXc2bG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/StyleGAN-nada/ZSSGAN/train.py --size 64 \\\n",
        "                --batch 2 \\\n",
        "                --n_sample 25 \\\n",
        "                --output_dir /content/out/mixing_0.9_412 \\\n",
        "                --lr 0.002 \\\n",
        "                --frozen_gen_ckpt /content/celebahq_100000.pt \\\n",
        "                --iter 501 \\\n",
        "                --style_img_dir /content/data \\\n",
        "                --auto_layer_k 4 \\\n",
        "                --auto_layer_iters 1 \\\n",
        "                --auto_layer_batch 2 \\\n",
        "                --output_interval 50 \\\n",
        "                --clip_models \"ViT-B/32\" \"ViT-B/16\" \\\n",
        "                --clip_model_weights 1.0 1.0 \\\n",
        "                --mixing 0.9 \\\n",
        "                --save_interval 10"
      ],
      "metadata": {
        "id": "YnvTy7hlHRau",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "087c0663-8e44-420e-88b2-143c1ab8c319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing networks...\n",
            "100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 173MiB/s]\n",
            "100%|███████████████████████████████████████| 244M/244M [00:04<00:00, 53.8MiB/s]\n",
            "100%|███████████████████████████████████████| 335M/335M [00:04<00:00, 71.6MiB/s]\n",
            "/content/StyleGAN-nada/ZSSGAN/op/conv2d_gradfix.py:88: UserWarning: conv2d_gradfix not supported on PyTorch 1.12.0+cu102. Falling back to torch.nn.functional.conv2d().\n",
            "  warnings.warn(\n",
            "Clip loss: 1.9541015625\n",
            "  0% 0/501 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchvision/utils.py:68: UserWarning: The parameter 'range' is deprecated since 0.12 and will be removed in 0.14. Please use 'value_range' instead.\n",
            "  warnings.warn(\n",
            "Clip loss: 2.05859375\n",
            "Clip loss: 1.7529296875\n",
            "Clip loss: 1.71875\n",
            "Clip loss: 1.5634765625\n",
            "Clip loss: 1.9345703125\n",
            "Clip loss: 1.6962890625\n",
            "Clip loss: 1.609375\n",
            "Clip loss: 1.830078125\n",
            "Clip loss: 1.7353515625\n",
            "Clip loss: 1.57421875\n",
            "Clip loss: 1.57421875\n",
            "Clip loss: 1.6484375\n",
            "Clip loss: 1.2861328125\n",
            "Clip loss: 1.501953125\n",
            "Clip loss: 1.3173828125\n",
            "Clip loss: 1.2109375\n",
            "Clip loss: 1.5986328125\n",
            "Clip loss: 1.8115234375\n",
            "Clip loss: 1.265625\n",
            "Clip loss: 1.337890625\n",
            "Clip loss: 1.8740234375\n",
            "Clip loss: 1.294921875\n",
            "Clip loss: 1.4375\n",
            "Clip loss: 1.201171875\n",
            "Clip loss: 1.37109375\n",
            "Clip loss: 1.16796875\n",
            "Clip loss: 1.154296875\n",
            "Clip loss: 1.259765625\n",
            "Clip loss: 1.556640625\n",
            "Clip loss: 1.25\n",
            "Clip loss: 1.1650390625\n",
            "Clip loss: 1.19140625\n",
            "Clip loss: 1.39453125\n",
            "Clip loss: 1.626953125\n",
            "Clip loss: 1.216796875\n",
            "Clip loss: 1.1396484375\n",
            "Clip loss: 1.189453125\n",
            "Clip loss: 1.1953125\n",
            "Clip loss: 1.53125\n",
            "Clip loss: 1.27734375\n",
            "Clip loss: 1.0703125\n",
            "Clip loss: 1.3134765625\n",
            "Clip loss: 1.32421875\n",
            "Clip loss: 1.265625\n",
            "Clip loss: 1.1923828125\n",
            "Clip loss: 1.107421875\n",
            "Clip loss: 1.271484375\n",
            "Clip loss: 1.1640625\n",
            "Clip loss: 1.18359375\n",
            "Clip loss: 1.2265625\n",
            "Clip loss: 1.1435546875\n",
            "Clip loss: 1.19140625\n",
            "Clip loss: 1.208984375\n",
            "Clip loss: 1.1298828125\n",
            "Clip loss: 1.1630859375\n",
            "Clip loss: 1.3505859375\n",
            "Clip loss: 1.1279296875\n",
            "Clip loss: 1.2275390625\n",
            "Clip loss: 1.1552734375\n",
            "Clip loss: 1.173828125\n",
            "Clip loss: 1.4130859375\n",
            "Clip loss: 1.2265625\n",
            "Clip loss: 1.041015625\n",
            "Clip loss: 1.037109375\n",
            "Clip loss: 1.166015625\n",
            "Clip loss: 1.2958984375\n",
            "Clip loss: 1.060546875\n",
            "Clip loss: 1.298828125\n",
            "Clip loss: 1.171875\n",
            "Clip loss: 1.19921875\n",
            "Clip loss: 1.037109375\n",
            "Clip loss: 1.216796875\n",
            "Clip loss: 1.1826171875\n",
            "Clip loss: 1.0361328125\n",
            "Clip loss: 1.1240234375\n",
            "Clip loss: 1.201171875\n",
            "Clip loss: 1.44921875\n",
            "Clip loss: 1.087890625\n",
            "Clip loss: 1.0458984375\n",
            "Clip loss: 1.119140625\n",
            "Clip loss: 1.05859375\n",
            "Clip loss: 1.404296875\n",
            "Clip loss: 1.3154296875\n",
            "Clip loss: 1.1796875\n",
            "Clip loss: 1.0712890625\n",
            "Clip loss: 1.18359375\n",
            "Clip loss: 1.123046875\n",
            "Clip loss: 1.1748046875\n",
            "Clip loss: 1.23046875\n",
            "Clip loss: 1.2119140625\n",
            "Clip loss: 1.28515625\n",
            "Clip loss: 1.1396484375\n",
            "Clip loss: 1.359375\n",
            "Clip loss: 1.1181640625\n",
            "Clip loss: 1.24609375\n",
            "Clip loss: 1.1083984375\n",
            "Clip loss: 1.427734375\n",
            "Clip loss: 1.146484375\n",
            "Clip loss: 1.154296875\n",
            "Clip loss: 1.0390625\n",
            "Clip loss: 0.99755859375\n",
            "Clip loss: 1.12890625\n",
            "Clip loss: 1.126953125\n",
            "Clip loss: 0.9990234375\n",
            "Clip loss: 1.25\n",
            "Clip loss: 1.09375\n",
            "Clip loss: 0.9619140625\n",
            "Clip loss: 1.1201171875\n",
            "Clip loss: 1.150390625\n",
            "Clip loss: 0.998046875\n",
            "Clip loss: 1.041015625\n",
            "Clip loss: 0.984375\n",
            "Clip loss: 1.09765625\n",
            "Clip loss: 1.05859375\n",
            "Clip loss: 0.84765625\n",
            "Clip loss: 1.08203125\n",
            "Clip loss: 1.033203125\n",
            "Clip loss: 1.31640625\n",
            "Clip loss: 1.080078125\n",
            "Clip loss: 0.9541015625\n",
            "Clip loss: 0.865234375\n",
            "Clip loss: 1.0771484375\n",
            "Clip loss: 0.7998046875\n",
            "Clip loss: 1.0546875\n",
            "Clip loss: 0.97265625\n",
            "Clip loss: 1.1865234375\n",
            "Clip loss: 0.9501953125\n",
            "Clip loss: 0.9033203125\n",
            "Clip loss: 1.0205078125\n",
            "Clip loss: 0.80029296875\n",
            "Clip loss: 0.76416015625\n",
            "Clip loss: 0.990234375\n",
            "Clip loss: 1.064453125\n",
            "Clip loss: 1.0029296875\n",
            "Clip loss: 0.77099609375\n",
            "Clip loss: 0.76025390625\n",
            "Clip loss: 0.640625\n",
            "Clip loss: 0.71484375\n",
            "Clip loss: 0.84765625\n",
            "Clip loss: 0.7275390625\n",
            "Clip loss: 0.7587890625\n",
            "Clip loss: 0.77197265625\n",
            "Clip loss: 0.6396484375\n",
            "Clip loss: 0.6728515625\n",
            "Clip loss: 0.77783203125\n",
            "Clip loss: 0.982421875\n",
            "Clip loss: 0.8125\n",
            "Clip loss: 0.9521484375\n",
            "Clip loss: 0.7578125\n",
            "Clip loss: 0.8974609375\n",
            "Clip loss: 0.9345703125\n",
            "Clip loss: 0.6728515625\n",
            "Clip loss: 0.9482421875\n",
            "Clip loss: 0.7919921875\n",
            "Clip loss: 0.7421875\n",
            "Clip loss: 0.9072265625\n",
            "Clip loss: 0.701171875\n",
            "Clip loss: 0.6640625\n",
            "Clip loss: 0.7353515625\n",
            "Clip loss: 0.96533203125\n",
            "Clip loss: 0.96484375\n",
            "Clip loss: 0.865234375\n",
            "Clip loss: 0.82421875\n",
            "Clip loss: 0.736328125\n",
            "Clip loss: 0.85302734375\n",
            "Clip loss: 0.78125\n",
            "Clip loss: 0.6728515625\n",
            "Clip loss: 0.93212890625\n",
            "Clip loss: 0.86328125\n",
            "Clip loss: 0.67333984375\n",
            "Clip loss: 0.91455078125\n",
            "Clip loss: 0.693359375\n",
            "Clip loss: 0.556640625\n",
            "Clip loss: 0.6572265625\n",
            "Clip loss: 0.6650390625\n",
            "Clip loss: 0.7197265625\n",
            "Clip loss: 0.6728515625\n",
            "Clip loss: 0.6904296875\n",
            "Clip loss: 0.619140625\n",
            "Clip loss: 0.822265625\n",
            "Clip loss: 0.79638671875\n",
            "Clip loss: 0.59423828125\n",
            "Clip loss: 0.65625\n",
            "Clip loss: 0.73046875\n",
            "Clip loss: 0.86328125\n",
            "Clip loss: 0.6552734375\n",
            "Clip loss: 0.58203125\n",
            "Clip loss: 0.7294921875\n",
            "Clip loss: 0.685546875\n",
            "Clip loss: 0.68505859375\n",
            "Clip loss: 0.66552734375\n",
            "Clip loss: 0.52294921875\n",
            "Clip loss: 0.82421875\n",
            "Clip loss: 0.7548828125\n",
            "Clip loss: 0.7412109375\n",
            "Clip loss: 0.8330078125\n",
            "Clip loss: 0.65478515625\n",
            "Clip loss: 0.6923828125\n",
            "Clip loss: 0.7626953125\n",
            "Clip loss: 0.9150390625\n",
            "Clip loss: 0.6103515625\n",
            "Clip loss: 0.7021484375\n",
            "Clip loss: 0.6162109375\n",
            "Clip loss: 0.76953125\n",
            "Clip loss: 0.6884765625\n",
            "Clip loss: 0.9140625\n",
            "Clip loss: 0.75341796875\n",
            "Clip loss: 0.74609375\n",
            "Clip loss: 0.66796875\n",
            "Clip loss: 0.5439453125\n",
            "Clip loss: 0.80712890625\n",
            "Clip loss: 0.7734375\n",
            "Clip loss: 0.7646484375\n",
            "Clip loss: 0.58056640625\n",
            "Clip loss: 0.63623046875\n",
            "Clip loss: 0.6787109375\n",
            "Clip loss: 0.6767578125\n",
            "Clip loss: 0.6572265625\n",
            "Clip loss: 0.49365234375\n",
            "Clip loss: 0.615234375\n",
            "Clip loss: 0.638671875\n",
            "Clip loss: 0.74951171875\n",
            "Clip loss: 0.6962890625\n",
            "Clip loss: 0.8125\n",
            "Clip loss: 0.7470703125\n",
            "Clip loss: 0.7421875\n",
            "Clip loss: 0.74609375\n",
            "Clip loss: 0.5986328125\n",
            "Clip loss: 0.83642578125\n",
            "Clip loss: 0.70068359375\n",
            "Clip loss: 0.6083984375\n",
            "Clip loss: 0.7294921875\n",
            "Clip loss: 0.8837890625\n",
            "Clip loss: 0.66650390625\n",
            "Clip loss: 0.708984375\n",
            "Clip loss: 0.880859375\n",
            "Clip loss: 0.609375\n",
            "Clip loss: 0.712890625\n",
            "Clip loss: 0.576171875\n",
            "Clip loss: 0.828125\n",
            "Clip loss: 0.65576171875\n",
            "Clip loss: 0.54248046875\n",
            "Clip loss: 0.6806640625\n",
            "Clip loss: 0.79541015625\n",
            "Clip loss: 0.75244140625\n",
            "Clip loss: 0.69189453125\n",
            "Clip loss: 0.525390625\n",
            "Clip loss: 0.5986328125\n",
            "Clip loss: 0.7431640625\n",
            "Clip loss: 0.7578125\n",
            "Clip loss: 0.5615234375\n",
            "Clip loss: 0.5966796875\n",
            "Clip loss: 0.7255859375\n",
            "Clip loss: 0.849609375\n",
            "Clip loss: 0.7353515625\n",
            "Clip loss: 0.716796875\n",
            "Clip loss: 0.50732421875\n",
            "Clip loss: 0.65869140625\n",
            "Clip loss: 0.677734375\n",
            "Clip loss: 0.77978515625\n",
            "Clip loss: 0.615234375\n",
            "Clip loss: 0.64892578125\n",
            "Clip loss: 0.759765625\n",
            "Clip loss: 0.83984375\n",
            "Clip loss: 0.6162109375\n",
            "Clip loss: 0.541015625\n",
            "Clip loss: 0.51904296875\n",
            "Clip loss: 0.6689453125\n",
            "Clip loss: 0.7275390625\n",
            "Clip loss: 0.6875\n",
            "Clip loss: 0.6435546875\n",
            "Clip loss: 0.7333984375\n",
            "Clip loss: 0.77734375\n",
            "Clip loss: 0.6904296875\n",
            "Clip loss: 0.568359375\n",
            "Clip loss: 0.68994140625\n",
            "Clip loss: 0.63671875\n",
            "Clip loss: 0.751953125\n",
            "Clip loss: 0.71728515625\n",
            "Clip loss: 0.7275390625\n",
            "Clip loss: 0.880859375\n",
            "Clip loss: 0.669921875\n",
            "Clip loss: 0.8447265625\n",
            "Clip loss: 0.61376953125\n",
            "Clip loss: 0.5283203125\n",
            "Clip loss: 0.6845703125\n",
            "Clip loss: 0.52294921875\n",
            "Clip loss: 0.572265625\n",
            "Clip loss: 0.69482421875\n",
            "Clip loss: 0.4765625\n",
            "Clip loss: 0.486572265625\n",
            "Clip loss: 0.492431640625\n",
            "Clip loss: 0.65576171875\n",
            "Clip loss: 0.6708984375\n",
            "Clip loss: 0.7119140625\n",
            "Clip loss: 0.72900390625\n",
            "Clip loss: 0.6298828125\n",
            "Clip loss: 0.54931640625\n",
            "Clip loss: 0.55517578125\n",
            "Clip loss: 0.6064453125\n",
            "Clip loss: 0.7275390625\n",
            "Clip loss: 0.619140625\n",
            "Clip loss: 0.5302734375\n",
            "Clip loss: 0.6171875\n",
            "Clip loss: 0.66455078125\n",
            "Clip loss: 0.8193359375\n",
            "Clip loss: 0.890625\n",
            "Clip loss: 0.4736328125\n",
            "Clip loss: 0.5546875\n",
            "Clip loss: 0.5634765625\n",
            "Clip loss: 0.78369140625\n",
            "Clip loss: 0.5322265625\n",
            "Clip loss: 0.5771484375\n",
            "Clip loss: 0.71875\n",
            "Clip loss: 0.69921875\n",
            "Clip loss: 0.6005859375\n",
            "Clip loss: 0.6201171875\n",
            "Clip loss: 0.6201171875\n",
            "Clip loss: 0.728515625\n",
            "Clip loss: 0.6279296875\n",
            "Clip loss: 0.70703125\n",
            "Clip loss: 0.634765625\n",
            "Clip loss: 0.61328125\n",
            "Clip loss: 0.5634765625\n",
            "Clip loss: 0.58203125\n",
            "Clip loss: 0.62353515625\n",
            "Clip loss: 0.488037109375\n",
            "Clip loss: 0.6142578125\n",
            "Clip loss: 0.6357421875\n",
            "Clip loss: 0.6865234375\n",
            "Clip loss: 0.63232421875\n",
            "Clip loss: 0.7333984375\n",
            "Clip loss: 0.66796875\n",
            "Clip loss: 0.5302734375\n",
            "Clip loss: 0.6552734375\n",
            "Clip loss: 0.79248046875\n",
            "Clip loss: 0.70068359375\n",
            "Clip loss: 0.7099609375\n",
            "Clip loss: 0.625\n",
            "Clip loss: 0.5986328125\n",
            "Clip loss: 0.482421875\n",
            "Clip loss: 0.69140625\n",
            "Clip loss: 0.6943359375\n",
            "Clip loss: 0.7529296875\n",
            "Clip loss: 0.65625\n",
            "Clip loss: 0.48193359375\n",
            "Clip loss: 0.6767578125\n",
            "Clip loss: 0.8662109375\n",
            "Clip loss: 0.66357421875\n",
            "Clip loss: 0.7080078125\n",
            "Clip loss: 0.5419921875\n",
            "Clip loss: 0.6455078125\n",
            "Clip loss: 0.82421875\n",
            "Clip loss: 0.7744140625\n",
            "Clip loss: 0.7548828125\n",
            "Clip loss: 0.5595703125\n",
            "Clip loss: 0.70703125\n",
            "Clip loss: 0.5263671875\n",
            "Clip loss: 0.6181640625\n",
            "Clip loss: 0.6650390625\n",
            "Clip loss: 0.58203125\n",
            "Clip loss: 0.6337890625\n",
            "Clip loss: 0.71142578125\n",
            "Clip loss: 0.81640625\n",
            "Clip loss: 0.67333984375\n",
            "Clip loss: 0.5771484375\n",
            "Clip loss: 0.6611328125\n",
            "Clip loss: 0.47705078125\n",
            "Clip loss: 0.703125\n",
            "Clip loss: 0.58642578125\n",
            "Clip loss: 0.73876953125\n",
            "Clip loss: 0.64404296875\n",
            "Clip loss: 0.5791015625\n",
            "Clip loss: 0.7021484375\n",
            "Clip loss: 0.60205078125\n",
            "Clip loss: 0.58203125\n",
            "Clip loss: 0.5361328125\n",
            "Clip loss: 0.587890625\n",
            "Clip loss: 0.58984375\n",
            "Clip loss: 0.6376953125\n",
            "Clip loss: 0.7587890625\n",
            "Clip loss: 0.62060546875\n",
            "Clip loss: 0.7353515625\n",
            "Clip loss: 0.5322265625\n",
            "Clip loss: 0.5966796875\n",
            "Clip loss: 0.62646484375\n",
            "Clip loss: 0.6025390625\n",
            "Clip loss: 0.662109375\n",
            "Clip loss: 0.5185546875\n",
            "Clip loss: 0.501953125\n",
            "Clip loss: 0.56982421875\n",
            "Clip loss: 0.60546875\n",
            "Clip loss: 0.751953125\n",
            "Clip loss: 0.463134765625\n",
            "Clip loss: 0.5234375\n",
            "Clip loss: 0.460693359375\n",
            "Clip loss: 0.67626953125\n",
            "Clip loss: 0.56640625\n",
            "Clip loss: 0.7001953125\n",
            "Clip loss: 0.6474609375\n",
            "Clip loss: 0.521484375\n",
            "Clip loss: 0.58203125\n",
            "Clip loss: 0.7265625\n",
            "Clip loss: 0.80029296875\n",
            "Clip loss: 0.701171875\n",
            "Clip loss: 0.5654296875\n",
            "Clip loss: 0.634765625\n",
            "Clip loss: 0.6826171875\n",
            "Clip loss: 0.48291015625\n",
            "Clip loss: 0.5361328125\n",
            "Clip loss: 0.5771484375\n",
            "Clip loss: 0.6103515625\n",
            "Clip loss: 0.595703125\n",
            "Clip loss: 0.634765625\n",
            "Clip loss: 0.68212890625\n",
            "Clip loss: 0.5126953125\n",
            "Clip loss: 0.6240234375\n",
            "Clip loss: 0.796875\n",
            "Clip loss: 0.734375\n",
            "Clip loss: 0.787109375\n",
            "Clip loss: 0.6142578125\n",
            "Clip loss: 0.7724609375\n",
            "Clip loss: 0.62939453125\n",
            "Clip loss: 0.6015625\n",
            "Clip loss: 0.479248046875\n",
            "Clip loss: 0.7373046875\n",
            "Clip loss: 0.60302734375\n",
            "Clip loss: 0.56591796875\n",
            "Clip loss: 0.611328125\n",
            "Clip loss: 0.65185546875\n",
            "Clip loss: 0.576171875\n",
            "Clip loss: 0.51904296875\n",
            "Clip loss: 0.62353515625\n",
            "Clip loss: 0.712890625\n",
            "Clip loss: 0.615234375\n",
            "Clip loss: 0.7373046875\n",
            "Clip loss: 0.7138671875\n",
            "Clip loss: 0.60205078125\n",
            "Clip loss: 0.56201171875\n",
            "Clip loss: 0.51171875\n",
            "Clip loss: 0.56689453125\n",
            "Clip loss: 0.7626953125\n",
            "Clip loss: 0.6748046875\n",
            "Clip loss: 0.5263671875\n",
            "Clip loss: 0.70166015625\n",
            "Clip loss: 0.70703125\n",
            "Clip loss: 0.5478515625\n",
            "Clip loss: 0.485595703125\n",
            "Clip loss: 0.54296875\n",
            "Clip loss: 0.673828125\n",
            "Clip loss: 0.7509765625\n",
            "Clip loss: 0.64404296875\n",
            "Clip loss: 0.64453125\n",
            "Clip loss: 0.5986328125\n",
            "Clip loss: 0.56103515625\n",
            "Clip loss: 0.7275390625\n",
            "Clip loss: 0.6708984375\n",
            "Clip loss: 0.6640625\n",
            "Clip loss: 0.626953125\n",
            "Clip loss: 0.67333984375\n",
            "Clip loss: 0.6171875\n",
            "Clip loss: 0.62841796875\n",
            "Clip loss: 0.448974609375\n",
            "Clip loss: 0.646484375\n",
            "Clip loss: 0.71484375\n",
            "Clip loss: 0.533203125\n",
            "Clip loss: 0.5947265625\n",
            "Clip loss: 0.6298828125\n",
            "Clip loss: 0.521484375\n",
            "Clip loss: 1.00390625\n",
            "Clip loss: 0.4453125\n",
            "Clip loss: 0.56982421875\n",
            "Clip loss: 0.6044921875\n",
            "Clip loss: 0.669921875\n",
            "Clip loss: 0.47314453125\n",
            "Clip loss: 0.5244140625\n",
            "Clip loss: 0.6279296875\n",
            "Clip loss: 0.6435546875\n",
            "Clip loss: 0.50439453125\n",
            "Clip loss: 0.61083984375\n",
            "Clip loss: 0.705078125\n",
            "Clip loss: 0.71826171875\n",
            "Clip loss: 0.5244140625\n",
            "Clip loss: 0.6044921875\n",
            "Clip loss: 0.61328125\n",
            "Clip loss: 0.59375\n",
            "Clip loss: 0.541015625\n",
            "Clip loss: 0.471435546875\n",
            "Clip loss: 0.63671875\n",
            "Clip loss: 0.646484375\n",
            "Clip loss: 0.5576171875\n",
            "Clip loss: 0.712890625\n",
            "Clip loss: 0.5283203125\n",
            "Clip loss: 0.42626953125\n",
            "Clip loss: 0.70703125\n",
            "Clip loss: 0.66259765625\n",
            "Clip loss: 0.78564453125\n",
            "Clip loss: 0.4892578125\n",
            "Clip loss: 0.5732421875\n",
            "Clip loss: 0.6103515625\n",
            "100% 501/501 [03:29<00:00,  2.39it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transfer learning to StyleGan With ADAM"
      ],
      "metadata": {
        "id": "s3WlGzKkzJ6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## non-leaking"
      ],
      "metadata": {
        "id": "vD-o5QWyKN52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "from torch import autograd\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from op import upfirdn2d\n",
        "\n",
        "\n",
        "class AdaptiveAugment:\n",
        "    def __init__(self, ada_aug_target, ada_aug_len, update_every, device):\n",
        "        self.ada_aug_target = ada_aug_target\n",
        "        self.ada_aug_len = ada_aug_len\n",
        "        self.update_every = update_every\n",
        "\n",
        "        self.ada_update = 0\n",
        "        self.ada_aug_buf = torch.tensor([0.0, 0.0], device=device)\n",
        "        self.r_t_stat = 0\n",
        "        self.ada_aug_p = 0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def tune(self, real_pred):\n",
        "        self.ada_aug_buf += torch.tensor(\n",
        "            (torch.sign(real_pred).sum().item(), real_pred.shape[0]),\n",
        "            device=real_pred.device,\n",
        "        )\n",
        "        self.ada_update += 1\n",
        "\n",
        "        if self.ada_update % self.update_every == 0:\n",
        "            self.ada_aug_buf = reduce_sum(self.ada_aug_buf)\n",
        "            pred_signs, n_pred = self.ada_aug_buf.tolist()\n",
        "\n",
        "            self.r_t_stat = pred_signs / n_pred\n",
        "\n",
        "            if self.r_t_stat > self.ada_aug_target:\n",
        "                sign = 1\n",
        "\n",
        "            else:\n",
        "                sign = -1\n",
        "\n",
        "            self.ada_aug_p += sign * n_pred / self.ada_aug_len\n",
        "            self.ada_aug_p = min(1, max(0, self.ada_aug_p))\n",
        "            self.ada_aug_buf.mul_(0)\n",
        "            self.ada_update = 0\n",
        "\n",
        "        return self.ada_aug_p\n",
        "\n",
        "\n",
        "SYM6 = (\n",
        "    0.015404109327027373,\n",
        "    0.0034907120842174702,\n",
        "    -0.11799011114819057,\n",
        "    -0.048311742585633,\n",
        "    0.4910559419267466,\n",
        "    0.787641141030194,\n",
        "    0.3379294217276218,\n",
        "    -0.07263752278646252,\n",
        "    -0.021060292512300564,\n",
        "    0.04472490177066578,\n",
        "    0.0017677118642428036,\n",
        "    -0.007800708325034148,\n",
        ")\n",
        "\n",
        "\n",
        "def translate_mat(t_x, t_y, device=\"cpu\"):\n",
        "    batch = t_x.shape[0]\n",
        "\n",
        "    mat = torch.eye(3, device=device).unsqueeze(0).repeat(batch, 1, 1)\n",
        "    translate = torch.stack((t_x, t_y), 1)\n",
        "    mat[:, :2, 2] = translate\n",
        "\n",
        "    return mat\n",
        "\n",
        "\n",
        "def rotate_mat(theta, device=\"cpu\"):\n",
        "    batch = theta.shape[0]\n",
        "\n",
        "    mat = torch.eye(3, device=device).unsqueeze(0).repeat(batch, 1, 1)\n",
        "    sin_t = torch.sin(theta)\n",
        "    cos_t = torch.cos(theta)\n",
        "    rot = torch.stack((cos_t, -sin_t, sin_t, cos_t), 1).view(batch, 2, 2)\n",
        "    mat[:, :2, :2] = rot\n",
        "\n",
        "    return mat\n",
        "\n",
        "\n",
        "def scale_mat(s_x, s_y, device=\"cpu\"):\n",
        "    batch = s_x.shape[0]\n",
        "\n",
        "    mat = torch.eye(3, device=device).unsqueeze(0).repeat(batch, 1, 1)\n",
        "    mat[:, 0, 0] = s_x\n",
        "    mat[:, 1, 1] = s_y\n",
        "\n",
        "    return mat\n",
        "\n",
        "\n",
        "def translate3d_mat(t_x, t_y, t_z):\n",
        "    batch = t_x.shape[0]\n",
        "\n",
        "    mat = torch.eye(4).unsqueeze(0).repeat(batch, 1, 1)\n",
        "    translate = torch.stack((t_x, t_y, t_z), 1)\n",
        "    mat[:, :3, 3] = translate\n",
        "\n",
        "    return mat\n",
        "\n",
        "\n",
        "def rotate3d_mat(axis, theta):\n",
        "    batch = theta.shape[0]\n",
        "\n",
        "    u_x, u_y, u_z = axis\n",
        "\n",
        "    eye = torch.eye(3).unsqueeze(0)\n",
        "    cross = torch.tensor([(0, -u_z, u_y), (u_z, 0, -u_x), (-u_y, u_x, 0)]).unsqueeze(0)\n",
        "    outer = torch.tensor(axis)\n",
        "    outer = (outer.unsqueeze(1) * outer).unsqueeze(0)\n",
        "\n",
        "    sin_t = torch.sin(theta).view(-1, 1, 1)\n",
        "    cos_t = torch.cos(theta).view(-1, 1, 1)\n",
        "\n",
        "    rot = cos_t * eye + sin_t * cross + (1 - cos_t) * outer\n",
        "\n",
        "    eye_4 = torch.eye(4).unsqueeze(0).repeat(batch, 1, 1)\n",
        "    eye_4[:, :3, :3] = rot\n",
        "\n",
        "    return eye_4\n",
        "\n",
        "\n",
        "def scale3d_mat(s_x, s_y, s_z):\n",
        "    batch = s_x.shape[0]\n",
        "\n",
        "    mat = torch.eye(4).unsqueeze(0).repeat(batch, 1, 1)\n",
        "    mat[:, 0, 0] = s_x\n",
        "    mat[:, 1, 1] = s_y\n",
        "    mat[:, 2, 2] = s_z\n",
        "\n",
        "    return mat\n",
        "\n",
        "\n",
        "def luma_flip_mat(axis, i):\n",
        "    batch = i.shape[0]\n",
        "\n",
        "    eye = torch.eye(4).unsqueeze(0).repeat(batch, 1, 1)\n",
        "    axis = torch.tensor(axis + (0,))\n",
        "    flip = 2 * torch.ger(axis, axis) * i.view(-1, 1, 1)\n",
        "\n",
        "    return eye - flip\n",
        "\n",
        "\n",
        "def saturation_mat(axis, i):\n",
        "    batch = i.shape[0]\n",
        "\n",
        "    eye = torch.eye(4).unsqueeze(0).repeat(batch, 1, 1)\n",
        "    axis = torch.tensor(axis + (0,))\n",
        "    axis = torch.ger(axis, axis)\n",
        "    saturate = axis + (eye - axis) * i.view(-1, 1, 1)\n",
        "\n",
        "    return saturate\n",
        "\n",
        "\n",
        "def lognormal_sample(size, mean=0, std=1, device=\"cpu\"):\n",
        "    return torch.empty(size, device=device).log_normal_(mean=mean, std=std)\n",
        "\n",
        "\n",
        "def category_sample(size, categories, device=\"cpu\"):\n",
        "    category = torch.tensor(categories, device=device)\n",
        "    sample = torch.randint(high=len(categories), size=(size,), device=device)\n",
        "\n",
        "    return category[sample]\n",
        "\n",
        "\n",
        "def uniform_sample(size, low, high, device=\"cpu\"):\n",
        "    return torch.empty(size, device=device).uniform_(low, high)\n",
        "\n",
        "\n",
        "def normal_sample(size, mean=0, std=1, device=\"cpu\"):\n",
        "    return torch.empty(size, device=device).normal_(mean, std)\n",
        "\n",
        "\n",
        "def bernoulli_sample(size, p, device=\"cpu\"):\n",
        "    return torch.empty(size, device=device).bernoulli_(p)\n",
        "\n",
        "\n",
        "def random_mat_apply(p, transform, prev, eye, device=\"cpu\"):\n",
        "    size = transform.shape[0]\n",
        "    select = bernoulli_sample(size, p, device=device).view(size, 1, 1)\n",
        "    select_transform = select * transform + (1 - select) * eye\n",
        "\n",
        "    return select_transform @ prev\n",
        "\n",
        "\n",
        "def sample_affine(p, size, height, width, device=\"cpu\"):\n",
        "    G = torch.eye(3, device=device).unsqueeze(0).repeat(size, 1, 1)\n",
        "    eye = G\n",
        "\n",
        "    # flip\n",
        "    param = category_sample(size, (0, 1))\n",
        "    Gc = scale_mat(1 - 2.0 * param, torch.ones(size), device=device)\n",
        "    G = random_mat_apply(p, Gc, G, eye, device=device)\n",
        "    # print('flip', G, scale_mat(1 - 2.0 * param, torch.ones(size)), sep='\\n')\n",
        "\n",
        "    # 90 rotate\n",
        "    param = category_sample(size, (0, 3))\n",
        "    Gc = rotate_mat(-math.pi / 2 * param, device=device)\n",
        "    G = random_mat_apply(p, Gc, G, eye, device=device)\n",
        "    # print('90 rotate', G, rotate_mat(-math.pi / 2 * param), sep='\\n')\n",
        "\n",
        "    # integer translate\n",
        "    param = uniform_sample((2, size), -0.125, 0.125)\n",
        "    param_height = torch.round(param[0] * height)\n",
        "    param_width = torch.round(param[1] * width)\n",
        "    Gc = translate_mat(param_width, param_height, device=device)\n",
        "    G = random_mat_apply(p, Gc, G, eye, device=device)\n",
        "    # print('integer translate', G, translate_mat(param_width, param_height), sep='\\n')\n",
        "\n",
        "    # isotropic scale\n",
        "    param = lognormal_sample(size, std=0.2 * math.log(2))\n",
        "    Gc = scale_mat(param, param, device=device)\n",
        "    G = random_mat_apply(p, Gc, G, eye, device=device)\n",
        "    # print('isotropic scale', G, scale_mat(param, param), sep='\\n')\n",
        "\n",
        "    p_rot = 1 - math.sqrt(1 - p)\n",
        "\n",
        "    # pre-rotate\n",
        "    param = uniform_sample(size, -math.pi, math.pi)\n",
        "    Gc = rotate_mat(-param, device=device)\n",
        "    G = random_mat_apply(p_rot, Gc, G, eye, device=device)\n",
        "    # print('pre-rotate', G, rotate_mat(-param), sep='\\n')\n",
        "\n",
        "    # anisotropic scale\n",
        "    param = lognormal_sample(size, std=0.2 * math.log(2))\n",
        "    Gc = scale_mat(param, 1 / param, device=device)\n",
        "    G = random_mat_apply(p, Gc, G, eye, device=device)\n",
        "    # print('anisotropic scale', G, scale_mat(param, 1 / param), sep='\\n')\n",
        "\n",
        "    # post-rotate\n",
        "    param = uniform_sample(size, -math.pi, math.pi)\n",
        "    Gc = rotate_mat(-param, device=device)\n",
        "    G = random_mat_apply(p_rot, Gc, G, eye, device=device)\n",
        "    # print('post-rotate', G, rotate_mat(-param), sep='\\n')\n",
        "\n",
        "    # fractional translate\n",
        "    param = normal_sample((2, size), std=0.125)\n",
        "    Gc = translate_mat(param[1] * width, param[0] * height, device=device)\n",
        "    G = random_mat_apply(p, Gc, G, eye, device=device)\n",
        "    # print('fractional translate', G, translate_mat(param, param), sep='\\n')\n",
        "\n",
        "    return G\n",
        "\n",
        "\n",
        "def sample_color(p, size):\n",
        "    C = torch.eye(4).unsqueeze(0).repeat(size, 1, 1)\n",
        "    eye = C\n",
        "    axis_val = 1 / math.sqrt(3)\n",
        "    axis = (axis_val, axis_val, axis_val)\n",
        "\n",
        "    # brightness\n",
        "    param = normal_sample(size, std=0.2)\n",
        "    Cc = translate3d_mat(param, param, param)\n",
        "    C = random_mat_apply(p, Cc, C, eye)\n",
        "\n",
        "    # contrast\n",
        "    param = lognormal_sample(size, std=0.5 * math.log(2))\n",
        "    Cc = scale3d_mat(param, param, param)\n",
        "    C = random_mat_apply(p, Cc, C, eye)\n",
        "\n",
        "    # luma flip\n",
        "    param = category_sample(size, (0, 1))\n",
        "    Cc = luma_flip_mat(axis, param)\n",
        "    C = random_mat_apply(p, Cc, C, eye)\n",
        "\n",
        "    # hue rotation\n",
        "    param = uniform_sample(size, -math.pi, math.pi)\n",
        "    Cc = rotate3d_mat(axis, param)\n",
        "    C = random_mat_apply(p, Cc, C, eye)\n",
        "\n",
        "    # saturation\n",
        "    param = lognormal_sample(size, std=1 * math.log(2))\n",
        "    Cc = saturation_mat(axis, param)\n",
        "    C = random_mat_apply(p, Cc, C, eye)\n",
        "\n",
        "    return C\n",
        "\n",
        "\n",
        "def make_grid(shape, x0, x1, y0, y1, device):\n",
        "    n, c, h, w = shape\n",
        "    grid = torch.empty(n, h, w, 3, device=device)\n",
        "    grid[:, :, :, 0] = torch.linspace(x0, x1, w, device=device)\n",
        "    grid[:, :, :, 1] = torch.linspace(y0, y1, h, device=device).unsqueeze(-1)\n",
        "    grid[:, :, :, 2] = 1\n",
        "\n",
        "    return grid\n",
        "\n",
        "\n",
        "def affine_grid(grid, mat):\n",
        "    n, h, w, _ = grid.shape\n",
        "    return (grid.view(n, h * w, 3) @ mat.transpose(1, 2)).view(n, h, w, 2)\n",
        "\n",
        "\n",
        "def get_padding(G, height, width, kernel_size):\n",
        "    device = G.device\n",
        "\n",
        "    cx = (width - 1) / 2\n",
        "    cy = (height - 1) / 2\n",
        "    cp = torch.tensor(\n",
        "        [(-cx, -cy, 1), (cx, -cy, 1), (cx, cy, 1), (-cx, cy, 1)], device=device\n",
        "    )\n",
        "    cp = G @ cp.T\n",
        "\n",
        "    pad_k = kernel_size // 4\n",
        "\n",
        "    pad = cp[:, :2, :].permute(1, 0, 2).flatten(1)\n",
        "    pad = torch.cat((-pad, pad)).max(1).values\n",
        "    pad = pad + torch.tensor([pad_k * 2 - cx, pad_k * 2 - cy] * 2, device=device)\n",
        "    pad = pad.max(torch.tensor([0, 0] * 2, device=device))\n",
        "    pad = pad.min(torch.tensor([width - 1, height - 1] * 2, device=device))\n",
        "\n",
        "    pad_x1, pad_y1, pad_x2, pad_y2 = pad.ceil().to(torch.int32)\n",
        "\n",
        "    return pad_x1, pad_x2, pad_y1, pad_y2\n",
        "\n",
        "\n",
        "def try_sample_affine_and_pad(img, p, kernel_size, G=None):\n",
        "    batch, _, height, width = img.shape\n",
        "\n",
        "    G_try = G\n",
        "\n",
        "    if G is None:\n",
        "        G_try = torch.inverse(sample_affine(p, batch, height, width))\n",
        "\n",
        "    pad_x1, pad_x2, pad_y1, pad_y2 = get_padding(G_try, height, width, kernel_size)\n",
        "\n",
        "    img_pad = F.pad(img, (pad_x1, pad_x2, pad_y1, pad_y2), mode=\"reflect\")\n",
        "\n",
        "    return img_pad, G_try, (pad_x1, pad_x2, pad_y1, pad_y2)\n",
        "\n",
        "\n",
        "class GridSampleForward(autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, grid):\n",
        "        out = F.grid_sample(\n",
        "            input, grid, mode=\"bilinear\", padding_mode=\"zeros\", align_corners=False\n",
        "        )\n",
        "        ctx.save_for_backward(input, grid)\n",
        "\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, grid = ctx.saved_tensors\n",
        "        grad_input, grad_grid = GridSampleBackward.apply(grad_output, input, grid)\n",
        "\n",
        "        return grad_input, grad_grid\n",
        "\n",
        "\n",
        "class GridSampleBackward(autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, grad_output, input, grid):\n",
        "        op = torch._C._jit_get_operation(\"aten::grid_sampler_2d_backward\")\n",
        "        op = op[0]\n",
        "        grad_input, grad_grid = op(grad_output, input, grid, 0, 0, False,(True, True))\n",
        "        ctx.save_for_backward(grid)\n",
        "\n",
        "        return grad_input, grad_grid\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_grad_input, grad_grad_grid):\n",
        "        (grid,) = ctx.saved_tensors\n",
        "        grad_grad_output = None\n",
        "\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            grad_grad_output = GridSampleForward.apply(grad_grad_input, grid)\n",
        "\n",
        "        return grad_grad_output, None, None\n",
        "\n",
        "\n",
        "grid_sample = GridSampleForward.apply\n",
        "\n",
        "\n",
        "def scale_mat_single(s_x, s_y):\n",
        "    return torch.tensor(((s_x, 0, 0), (0, s_y, 0), (0, 0, 1)), dtype=torch.float32)\n",
        "\n",
        "\n",
        "def translate_mat_single(t_x, t_y):\n",
        "    return torch.tensor(((1, 0, t_x), (0, 1, t_y), (0, 0, 1)), dtype=torch.float32)\n",
        "\n",
        "\n",
        "def random_apply_affine(img, p, G=None, antialiasing_kernel=SYM6):\n",
        "    kernel = antialiasing_kernel\n",
        "    len_k = len(kernel)\n",
        "\n",
        "    kernel = torch.as_tensor(kernel).to(img)\n",
        "    # kernel = torch.ger(kernel, kernel).to(img)\n",
        "    kernel_flip = torch.flip(kernel, (0,))\n",
        "\n",
        "    img_pad, G, (pad_x1, pad_x2, pad_y1, pad_y2) = try_sample_affine_and_pad(\n",
        "        img, p, len_k, G\n",
        "    )\n",
        "\n",
        "    G_inv = (\n",
        "        translate_mat_single((pad_x1 - pad_x2).item() / 2, (pad_y1 - pad_y2).item() / 2)\n",
        "        @ G\n",
        "    )\n",
        "    up_pad = (\n",
        "        (len_k + 2 - 1) // 2,\n",
        "        (len_k - 2) // 2,\n",
        "        (len_k + 2 - 1) // 2,\n",
        "        (len_k - 2) // 2,\n",
        "    )\n",
        "    img_2x = upfirdn2d(img_pad, kernel.unsqueeze(0), up=(2, 1), pad=(*up_pad[:2], 0, 0))\n",
        "    img_2x = upfirdn2d(img_2x, kernel.unsqueeze(1), up=(1, 2), pad=(0, 0, *up_pad[2:]))\n",
        "    G_inv = scale_mat_single(2, 2) @ G_inv @ scale_mat_single(1 / 2, 1 / 2)\n",
        "    G_inv = translate_mat_single(-0.5, -0.5) @ G_inv @ translate_mat_single(0.5, 0.5)\n",
        "    batch_size, channel, height, width = img.shape\n",
        "    pad_k = len_k // 4\n",
        "    shape = (batch_size, channel, (height + pad_k * 2) * 2, (width + pad_k * 2) * 2)\n",
        "    G_inv = (\n",
        "        scale_mat_single(2 / img_2x.shape[3], 2 / img_2x.shape[2])\n",
        "        @ G_inv\n",
        "        @ scale_mat_single(1 / (2 / shape[3]), 1 / (2 / shape[2]))\n",
        "    )\n",
        "    grid = F.affine_grid(G_inv[:, :2, :].to(img_2x), shape, align_corners=False)\n",
        "    img_affine = grid_sample(img_2x, grid)\n",
        "    d_p = -pad_k * 2\n",
        "    down_pad = (\n",
        "        d_p + (len_k - 2 + 1) // 2,\n",
        "        d_p + (len_k - 2) // 2,\n",
        "        d_p + (len_k - 2 + 1) // 2,\n",
        "        d_p + (len_k - 2) // 2,\n",
        "    )\n",
        "    img_down = upfirdn2d(\n",
        "        img_affine, kernel_flip.unsqueeze(0), down=(2, 1), pad=(*down_pad[:2], 0, 0)\n",
        "    )\n",
        "    img_down = upfirdn2d(\n",
        "        img_down, kernel_flip.unsqueeze(1), down=(1, 2), pad=(0, 0, *down_pad[2:])\n",
        "    )\n",
        "\n",
        "    return img_down, G\n",
        "\n",
        "\n",
        "def apply_color(img, mat):\n",
        "    batch = img.shape[0]\n",
        "    img = img.permute(0, 2, 3, 1)\n",
        "    mat_mul = mat[:, :3, :3].transpose(1, 2).view(batch, 1, 3, 3)\n",
        "    mat_add = mat[:, :3, 3].view(batch, 1, 1, 3)\n",
        "    img = img @ mat_mul + mat_add\n",
        "    img = img.permute(0, 3, 1, 2)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def random_apply_color(img, p, C=None):\n",
        "    if C is None:\n",
        "        C = sample_color(p, img.shape[0])\n",
        "\n",
        "    img = apply_color(img, C.to(img))\n",
        "\n",
        "    return img, C\n",
        "\n",
        "\n",
        "def augment(img, p, transform_matrix=(None, None)):\n",
        "    img, G = random_apply_affine(img, p, transform_matrix[0])\n",
        "    img, C = random_apply_color(img, p, transform_matrix[1])\n",
        "\n",
        "    return img, (G, C)\n",
        "\n",
        "import os\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, path, transform):\n",
        "        self.filenames = os.listdir(path)\n",
        "        self.filenames = [os.path.join(path, f) for f in self.filenames]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = Image.open(self.filenames[index])\n",
        "        image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)"
      ],
      "metadata": {
        "id": "MasL97a8KMnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distributed"
      ],
      "metadata": {
        "id": "lIvTIH0aKVdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "from torch import distributed as dist\n",
        "from torch.utils.data.sampler import Sampler\n",
        "\n",
        "\n",
        "def get_rank():\n",
        "    if not dist.is_available():\n",
        "        return 0\n",
        "\n",
        "    if not dist.is_initialized():\n",
        "        return 0\n",
        "\n",
        "    return dist.get_rank()\n",
        "\n",
        "\n",
        "def synchronize():\n",
        "    if not dist.is_available():\n",
        "        return\n",
        "\n",
        "    if not dist.is_initialized():\n",
        "        return\n",
        "\n",
        "    world_size = dist.get_world_size()\n",
        "\n",
        "    if world_size == 1:\n",
        "        return\n",
        "\n",
        "    dist.barrier()\n",
        "\n",
        "\n",
        "def get_world_size():\n",
        "    if not dist.is_available():\n",
        "        return 1\n",
        "\n",
        "    if not dist.is_initialized():\n",
        "        return 1\n",
        "\n",
        "    return dist.get_world_size()\n",
        "\n",
        "\n",
        "def reduce_sum(tensor):\n",
        "    if not dist.is_available():\n",
        "        return tensor\n",
        "\n",
        "    if not dist.is_initialized():\n",
        "        return tensor\n",
        "\n",
        "    tensor = tensor.clone()\n",
        "    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n",
        "\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def gather_grad(params):\n",
        "    world_size = get_world_size()\n",
        "\n",
        "    if world_size == 1:\n",
        "        return\n",
        "\n",
        "    for param in params:\n",
        "        if param.grad is not None:\n",
        "            dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)\n",
        "            param.grad.data.div_(world_size)\n",
        "\n",
        "\n",
        "def all_gather(data):\n",
        "    world_size = get_world_size()\n",
        "\n",
        "    if world_size == 1:\n",
        "        return [data]\n",
        "\n",
        "    buffer = pickle.dumps(data)\n",
        "    storage = torch.ByteStorage.from_buffer(buffer)\n",
        "    tensor = torch.ByteTensor(storage).to('cuda')\n",
        "\n",
        "    local_size = torch.IntTensor([tensor.numel()]).to('cuda')\n",
        "    size_list = [torch.IntTensor([0]).to('cuda') for _ in range(world_size)]\n",
        "    dist.all_gather(size_list, local_size)\n",
        "    size_list = [int(size.item()) for size in size_list]\n",
        "    max_size = max(size_list)\n",
        "\n",
        "    tensor_list = []\n",
        "    for _ in size_list:\n",
        "        tensor_list.append(torch.ByteTensor(size=(max_size,)).to('cuda'))\n",
        "\n",
        "    if local_size != max_size:\n",
        "        padding = torch.ByteTensor(size=(max_size - local_size,)).to('cuda')\n",
        "        tensor = torch.cat((tensor, padding), 0)\n",
        "\n",
        "    dist.all_gather(tensor_list, tensor)\n",
        "\n",
        "    data_list = []\n",
        "\n",
        "    for size, tensor in zip(size_list, tensor_list):\n",
        "        buffer = tensor.cpu().numpy().tobytes()[:size]\n",
        "        data_list.append(pickle.loads(buffer))\n",
        "\n",
        "    return data_list\n",
        "\n",
        "\n",
        "def reduce_loss_dict(loss_dict):\n",
        "    world_size = get_world_size()\n",
        "\n",
        "    if world_size < 2:\n",
        "        return loss_dict\n",
        "\n",
        "    with torch.no_grad():\n",
        "        keys = []\n",
        "        losses = []\n",
        "\n",
        "        for k in sorted(loss_dict.keys()):\n",
        "            keys.append(k)\n",
        "            losses.append(loss_dict[k])\n",
        "\n",
        "        losses = torch.stack(losses, 0)\n",
        "        dist.reduce(losses, dst=0)\n",
        "\n",
        "        if dist.get_rank() == 0:\n",
        "            losses /= world_size\n",
        "\n",
        "        reduced_losses = {k: v for k, v in zip(keys, losses)}\n",
        "\n",
        "    return reduced_losses"
      ],
      "metadata": {
        "id": "7NjIhyltJ4uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## StyleGan2"
      ],
      "metadata": {
        "id": "VrmzbB1e7ekp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ABS3VC-GWpn"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from op import FusedLeakyReLU, fused_leaky_relu, upfirdn2d, conv2d_gradfix\n",
        "\n",
        "\n",
        "class PixelNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input * torch.rsqrt(torch.mean(input ** 2, dim=1, keepdim=True) + 1e-8)\n",
        "\n",
        "\n",
        "def make_kernel(k):\n",
        "    k = torch.tensor(k, dtype=torch.float32)\n",
        "\n",
        "    if k.ndim == 1:\n",
        "        k = k[None, :] * k[:, None]\n",
        "\n",
        "    k /= k.sum()\n",
        "\n",
        "    return k\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, kernel, factor=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.factor = factor\n",
        "        kernel = make_kernel(kernel) * (factor ** 2)\n",
        "        self.register_buffer(\"kernel\", kernel)\n",
        "\n",
        "        p = kernel.shape[0] - factor\n",
        "\n",
        "        pad0 = (p + 1) // 2 + factor - 1\n",
        "        pad1 = p // 2\n",
        "\n",
        "        self.pad = (pad0, pad1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = upfirdn2d(input, self.kernel, up=self.factor, down=1, pad=self.pad)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, kernel, factor=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.factor = factor\n",
        "        kernel = make_kernel(kernel)\n",
        "        self.register_buffer(\"kernel\", kernel)\n",
        "\n",
        "        p = kernel.shape[0] - factor\n",
        "\n",
        "        pad0 = (p + 1) // 2\n",
        "        pad1 = p // 2\n",
        "\n",
        "        self.pad = (pad0, pad1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = upfirdn2d(input, self.kernel, up=1, down=self.factor, pad=self.pad)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Blur(nn.Module):\n",
        "    def __init__(self, kernel, pad, upsample_factor=1):\n",
        "        super().__init__()\n",
        "\n",
        "        kernel = make_kernel(kernel)\n",
        "\n",
        "        if upsample_factor > 1:\n",
        "            kernel = kernel * (upsample_factor ** 2)\n",
        "\n",
        "        self.register_buffer(\"kernel\", kernel)\n",
        "\n",
        "        self.pad = pad\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = upfirdn2d(input, self.kernel, pad=self.pad)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class EqualConv2d(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channel, out_channel, kernel_size, stride=1, padding=0, bias=True\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.weight = nn.Parameter(\n",
        "            torch.randn(out_channel, in_channel, kernel_size, kernel_size)\n",
        "        )\n",
        "        self.scale = 1 / math.sqrt(in_channel * kernel_size ** 2)\n",
        "\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(out_channel))\n",
        "\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = conv2d_gradfix.conv2d(\n",
        "            input,\n",
        "            self.weight * self.scale,\n",
        "            bias=self.bias,\n",
        "            stride=self.stride,\n",
        "            padding=self.padding,\n",
        "        )\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (\n",
        "            f\"{self.__class__.__name__}({self.weight.shape[1]}, {self.weight.shape[0]},\"\n",
        "            f\" {self.weight.shape[2]}, stride={self.stride}, padding={self.padding})\"\n",
        "        )\n",
        "\n",
        "\n",
        "class EqualLinear(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_dim, out_dim, bias=True, bias_init=0, lr_mul=1, activation=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.weight = nn.Parameter(torch.randn(out_dim, in_dim).div_(lr_mul))\n",
        "\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(out_dim).fill_(bias_init))\n",
        "\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "        self.activation = activation\n",
        "\n",
        "        self.scale = (1 / math.sqrt(in_dim)) * lr_mul\n",
        "        self.lr_mul = lr_mul\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.activation:\n",
        "            out = F.linear(input, self.weight * self.scale)\n",
        "            out = fused_leaky_relu(out, self.bias * self.lr_mul)\n",
        "\n",
        "        else:\n",
        "            out = F.linear(\n",
        "                input, self.weight * self.scale, bias=self.bias * self.lr_mul\n",
        "            )\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (\n",
        "            f\"{self.__class__.__name__}({self.weight.shape[1]}, {self.weight.shape[0]})\"\n",
        "        )\n",
        "\n",
        "\n",
        "class ModulatedConv2d(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channel,\n",
        "        out_channel,\n",
        "        kernel_size,\n",
        "        style_dim,\n",
        "        demodulate=True,\n",
        "        upsample=False,\n",
        "        downsample=False,\n",
        "        blur_kernel=[1, 3, 3, 1],\n",
        "        fused=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.eps = 1e-8\n",
        "        self.kernel_size = kernel_size\n",
        "        self.in_channel = in_channel\n",
        "        self.out_channel = out_channel\n",
        "        self.upsample = upsample\n",
        "        self.downsample = downsample\n",
        "\n",
        "        if upsample:\n",
        "            factor = 2\n",
        "            p = (len(blur_kernel) - factor) - (kernel_size - 1)\n",
        "            pad0 = (p + 1) // 2 + factor - 1\n",
        "            pad1 = p // 2 + 1\n",
        "\n",
        "            self.blur = Blur(blur_kernel, pad=(pad0, pad1), upsample_factor=factor)\n",
        "\n",
        "        if downsample:\n",
        "            factor = 2\n",
        "            p = (len(blur_kernel) - factor) + (kernel_size - 1)\n",
        "            pad0 = (p + 1) // 2\n",
        "            pad1 = p // 2\n",
        "\n",
        "            self.blur = Blur(blur_kernel, pad=(pad0, pad1))\n",
        "\n",
        "        fan_in = in_channel * kernel_size ** 2\n",
        "        self.scale = 1 / math.sqrt(fan_in)\n",
        "        self.padding = kernel_size // 2\n",
        "\n",
        "        self.weight = nn.Parameter(\n",
        "            torch.randn(1, out_channel, in_channel, kernel_size, kernel_size)\n",
        "        )\n",
        "\n",
        "        self.modulation = EqualLinear(style_dim, in_channel, bias_init=1)\n",
        "\n",
        "        self.demodulate = demodulate\n",
        "        self.fused = fused\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (\n",
        "            f\"{self.__class__.__name__}({self.in_channel}, {self.out_channel}, {self.kernel_size}, \"\n",
        "            f\"upsample={self.upsample}, downsample={self.downsample})\"\n",
        "        )\n",
        "\n",
        "    def forward(self, input, style):\n",
        "        batch, in_channel, height, width = input.shape\n",
        "\n",
        "        if not self.fused:\n",
        "            weight = self.scale * self.weight.squeeze(0)\n",
        "            style = self.modulation(style)\n",
        "\n",
        "            if self.demodulate:\n",
        "                w = weight.unsqueeze(0) * style.view(batch, 1, in_channel, 1, 1)\n",
        "                dcoefs = (w.square().sum((2, 3, 4)) + 1e-8).rsqrt()\n",
        "\n",
        "            input = input * style.reshape(batch, in_channel, 1, 1)\n",
        "\n",
        "            if self.upsample:\n",
        "                weight = weight.transpose(0, 1)\n",
        "                out = conv2d_gradfix.conv_transpose2d(\n",
        "                    input, weight, padding=0, stride=2\n",
        "                )\n",
        "                out = self.blur(out)\n",
        "\n",
        "            elif self.downsample:\n",
        "                input = self.blur(input)\n",
        "                out = conv2d_gradfix.conv2d(input, weight, padding=0, stride=2)\n",
        "\n",
        "            else:\n",
        "                out = conv2d_gradfix.conv2d(input, weight, padding=self.padding)\n",
        "\n",
        "            if self.demodulate:\n",
        "                out = out * dcoefs.view(batch, -1, 1, 1)\n",
        "\n",
        "            return out\n",
        "\n",
        "        style = self.modulation(style).view(batch, 1, in_channel, 1, 1)\n",
        "        weight = self.scale * self.weight * style\n",
        "\n",
        "        if self.demodulate:\n",
        "            demod = torch.rsqrt(weight.pow(2).sum([2, 3, 4]) + 1e-8)\n",
        "            weight = weight * demod.view(batch, self.out_channel, 1, 1, 1)\n",
        "\n",
        "        weight = weight.view(\n",
        "            batch * self.out_channel, in_channel, self.kernel_size, self.kernel_size\n",
        "        )\n",
        "\n",
        "        if self.upsample:\n",
        "            input = input.view(1, batch * in_channel, height, width)\n",
        "            weight = weight.view(\n",
        "                batch, self.out_channel, in_channel, self.kernel_size, self.kernel_size\n",
        "            )\n",
        "            weight = weight.transpose(1, 2).reshape(\n",
        "                batch * in_channel, self.out_channel, self.kernel_size, self.kernel_size\n",
        "            )\n",
        "            out = conv2d_gradfix.conv_transpose2d(\n",
        "                input, weight, padding=0, stride=2, groups=batch\n",
        "            )\n",
        "            _, _, height, width = out.shape\n",
        "            out = out.view(batch, self.out_channel, height, width)\n",
        "            out = self.blur(out)\n",
        "\n",
        "        elif self.downsample:\n",
        "            input = self.blur(input)\n",
        "            _, _, height, width = input.shape\n",
        "            input = input.view(1, batch * in_channel, height, width)\n",
        "            out = conv2d_gradfix.conv2d(\n",
        "                input, weight, padding=0, stride=2, groups=batch\n",
        "            )\n",
        "            _, _, height, width = out.shape\n",
        "            out = out.view(batch, self.out_channel, height, width)\n",
        "\n",
        "        else:\n",
        "            input = input.view(1, batch * in_channel, height, width)\n",
        "            out = conv2d_gradfix.conv2d(\n",
        "                input, weight, padding=self.padding, groups=batch\n",
        "            )\n",
        "            _, _, height, width = out.shape\n",
        "            out = out.view(batch, self.out_channel, height, width)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class NoiseInjection(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.weight = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, image, noise=None):\n",
        "        if noise is None:\n",
        "            batch, _, height, width = image.shape\n",
        "            noise = image.new_empty(batch, 1, height, width).normal_()\n",
        "\n",
        "        return image + self.weight * noise\n",
        "\n",
        "\n",
        "class ConstantInput(nn.Module):\n",
        "    def __init__(self, channel, size=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input = nn.Parameter(torch.randn(1, channel, size, size))\n",
        "\n",
        "    def forward(self, input):\n",
        "        batch = input.shape[0]\n",
        "        out = self.input.repeat(batch, 1, 1, 1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class StyledConv(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channel,\n",
        "        out_channel,\n",
        "        kernel_size,\n",
        "        style_dim,\n",
        "        upsample=False,\n",
        "        blur_kernel=[1, 3, 3, 1],\n",
        "        demodulate=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = ModulatedConv2d(\n",
        "            in_channel,\n",
        "            out_channel,\n",
        "            kernel_size,\n",
        "            style_dim,\n",
        "            upsample=upsample,\n",
        "            blur_kernel=blur_kernel,\n",
        "            demodulate=demodulate,\n",
        "        )\n",
        "\n",
        "        self.noise = NoiseInjection()\n",
        "        # self.bias = nn.Parameter(torch.zeros(1, out_channel, 1, 1))\n",
        "        # self.activate = ScaledLeakyReLU(0.2)\n",
        "        self.activate = FusedLeakyReLU(out_channel)\n",
        "\n",
        "    def forward(self, input, style, noise=None):\n",
        "        out = self.conv(input, style)\n",
        "        out = self.noise(out, noise=noise)\n",
        "        # out = out + self.bias\n",
        "        out = self.activate(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ToRGB(nn.Module):\n",
        "    def __init__(self, in_channel, style_dim, upsample=True, blur_kernel=[1, 3, 3, 1]):\n",
        "        super().__init__()\n",
        "\n",
        "        if upsample:\n",
        "            self.upsample = Upsample(blur_kernel)\n",
        "\n",
        "        self.conv = ModulatedConv2d(in_channel, 3, 1, style_dim, demodulate=False)\n",
        "        self.bias = nn.Parameter(torch.zeros(1, 3, 1, 1))\n",
        "\n",
        "    def forward(self, input, style, skip=None):\n",
        "        out = self.conv(input, style)\n",
        "        out = out + self.bias\n",
        "\n",
        "        if skip is not None:\n",
        "            skip = self.upsample(skip)\n",
        "\n",
        "            out = out + skip\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        size,\n",
        "        style_dim,\n",
        "        n_mlp,\n",
        "        channel_multiplier=2,\n",
        "        blur_kernel=[1, 3, 3, 1],\n",
        "        lr_mlp=0.01,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.size = size\n",
        "\n",
        "        self.style_dim = style_dim\n",
        "\n",
        "        layers = [PixelNorm()]\n",
        "\n",
        "        for i in range(n_mlp):\n",
        "            layers.append(\n",
        "                EqualLinear(\n",
        "                    style_dim, style_dim, lr_mul=lr_mlp, activation=\"fused_lrelu\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.style = nn.Sequential(*layers)\n",
        "\n",
        "        self.channels = {\n",
        "            4: 512,\n",
        "            8: 512,\n",
        "            16: 512,\n",
        "            32: 512,\n",
        "            64: 256 * channel_multiplier,\n",
        "            128: 128 * channel_multiplier,\n",
        "            256: 64 * channel_multiplier,\n",
        "            512: 32 * channel_multiplier,\n",
        "            1024: 16 * channel_multiplier,\n",
        "        }\n",
        "\n",
        "        self.input = ConstantInput(self.channels[4])\n",
        "        self.conv1 = StyledConv(\n",
        "            self.channels[4], self.channels[4], 3, style_dim, blur_kernel=blur_kernel\n",
        "        )\n",
        "        self.to_rgb1 = ToRGB(self.channels[4], style_dim, upsample=False)\n",
        "\n",
        "        self.log_size = int(math.log(size, 2))\n",
        "        self.num_layers = (self.log_size - 2) * 2 + 1\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.upsamples = nn.ModuleList()\n",
        "        self.to_rgbs = nn.ModuleList()\n",
        "        self.noises = nn.Module()\n",
        "\n",
        "        in_channel = self.channels[4]\n",
        "\n",
        "        for layer_idx in range(self.num_layers):\n",
        "            res = (layer_idx + 5) // 2\n",
        "            shape = [1, 1, 2 ** res, 2 ** res]\n",
        "            self.noises.register_buffer(f\"noise_{layer_idx}\", torch.randn(*shape))\n",
        "\n",
        "        for i in range(3, self.log_size + 1):\n",
        "            out_channel = self.channels[2 ** i]\n",
        "\n",
        "            self.convs.append(\n",
        "                StyledConv(\n",
        "                    in_channel,\n",
        "                    out_channel,\n",
        "                    3,\n",
        "                    style_dim,\n",
        "                    upsample=True,\n",
        "                    blur_kernel=blur_kernel,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            self.convs.append(\n",
        "                StyledConv(\n",
        "                    out_channel, out_channel, 3, style_dim, blur_kernel=blur_kernel\n",
        "                )\n",
        "            )\n",
        "\n",
        "            self.to_rgbs.append(ToRGB(out_channel, style_dim))\n",
        "\n",
        "            in_channel = out_channel\n",
        "\n",
        "        self.n_latent = self.log_size * 2 - 2\n",
        "\n",
        "    def make_noise(self):\n",
        "        device = self.input.input.device\n",
        "\n",
        "        noises = [torch.randn(1, 1, 2 ** 2, 2 ** 2, device=device)]\n",
        "\n",
        "        for i in range(3, self.log_size + 1):\n",
        "            for _ in range(2):\n",
        "                noises.append(torch.randn(1, 1, 2 ** i, 2 ** i, device=device))\n",
        "\n",
        "        return noises\n",
        "\n",
        "    def mean_latent(self, n_latent):\n",
        "        latent_in = torch.randn(\n",
        "            n_latent, self.style_dim, device=self.input.input.device\n",
        "        )\n",
        "        latent = self.style(latent_in).mean(0, keepdim=True)\n",
        "\n",
        "        return latent\n",
        "\n",
        "    def get_latent(self, input):\n",
        "        return self.style(input)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        styles,\n",
        "        return_latents=False,\n",
        "        inject_index=None,\n",
        "        truncation=1,\n",
        "        truncation_latent=None,\n",
        "        input_is_latent=False,\n",
        "        noise=None,\n",
        "        randomize_noise=True,\n",
        "    ):\n",
        "        if not input_is_latent:\n",
        "            styles = [self.style(s) for s in styles]\n",
        "\n",
        "        if noise is None:\n",
        "            if randomize_noise:\n",
        "                noise = [None] * self.num_layers\n",
        "            else:\n",
        "                noise = [\n",
        "                    getattr(self.noises, f\"noise_{i}\") for i in range(self.num_layers)\n",
        "                ]\n",
        "\n",
        "        if truncation < 1:\n",
        "            style_t = []\n",
        "\n",
        "            for style in styles:\n",
        "                style_t.append(\n",
        "                    truncation_latent + truncation * (style - truncation_latent)\n",
        "                )\n",
        "\n",
        "            styles = style_t\n",
        "\n",
        "        if len(styles) < 2:\n",
        "            inject_index = self.n_latent\n",
        "\n",
        "            if styles[0].ndim < 3:\n",
        "                latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n",
        "\n",
        "            else:\n",
        "                latent = styles[0]\n",
        "\n",
        "        else:\n",
        "            if inject_index is None:\n",
        "                inject_index = random.randint(1, self.n_latent - 1)\n",
        "\n",
        "            latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n",
        "            latent2 = styles[1].unsqueeze(1).repeat(1, self.n_latent - inject_index, 1)\n",
        "\n",
        "            latent = torch.cat([latent, latent2], 1)\n",
        "\n",
        "        out = self.input(latent)\n",
        "        out = self.conv1(out, latent[:, 0], noise=noise[0])\n",
        "\n",
        "        skip = self.to_rgb1(out, latent[:, 1])\n",
        "\n",
        "        i = 1\n",
        "        for conv1, conv2, noise1, noise2, to_rgb in zip(\n",
        "            self.convs[::2], self.convs[1::2], noise[1::2], noise[2::2], self.to_rgbs\n",
        "        ):\n",
        "            out = conv1(out, latent[:, i], noise=noise1)\n",
        "            out = conv2(out, latent[:, i + 1], noise=noise2)\n",
        "            skip = to_rgb(out, latent[:, i + 2], skip)\n",
        "\n",
        "            i += 2\n",
        "\n",
        "        image = skip\n",
        "\n",
        "        if return_latents:\n",
        "            return image, latent\n",
        "\n",
        "        else:\n",
        "            return image, None\n",
        "\n",
        "\n",
        "class ConvLayer(nn.Sequential):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channel,\n",
        "        out_channel,\n",
        "        kernel_size,\n",
        "        downsample=False,\n",
        "        blur_kernel=[1, 3, 3, 1],\n",
        "        bias=True,\n",
        "        activate=True,\n",
        "    ):\n",
        "        layers = []\n",
        "\n",
        "        if downsample:\n",
        "            factor = 2\n",
        "            p = (len(blur_kernel) - factor) + (kernel_size - 1)\n",
        "            pad0 = (p + 1) // 2\n",
        "            pad1 = p // 2\n",
        "\n",
        "            layers.append(Blur(blur_kernel, pad=(pad0, pad1)))\n",
        "\n",
        "            stride = 2\n",
        "            self.padding = 0\n",
        "\n",
        "        else:\n",
        "            stride = 1\n",
        "            self.padding = kernel_size // 2\n",
        "\n",
        "        layers.append(\n",
        "            EqualConv2d(\n",
        "                in_channel,\n",
        "                out_channel,\n",
        "                kernel_size,\n",
        "                padding=self.padding,\n",
        "                stride=stride,\n",
        "                bias=bias and not activate,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        if activate:\n",
        "            layers.append(FusedLeakyReLU(out_channel, bias=bias))\n",
        "\n",
        "        super().__init__(*layers)\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, blur_kernel=[1, 3, 3, 1]):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = ConvLayer(in_channel, in_channel, 3)\n",
        "        self.conv2 = ConvLayer(in_channel, out_channel, 3, downsample=True)\n",
        "\n",
        "        self.skip = ConvLayer(\n",
        "            in_channel, out_channel, 1, downsample=True, activate=False, bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.conv1(input)\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        skip = self.skip(input)\n",
        "        out = (out + skip) / math.sqrt(2)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, size, channel_multiplier=2, blur_kernel=[1, 3, 3, 1]):\n",
        "        super().__init__()\n",
        "\n",
        "        channels = {\n",
        "            4: 512,\n",
        "            8: 512,\n",
        "            16: 512,\n",
        "            32: 512,\n",
        "            64: 256 * channel_multiplier,\n",
        "            128: 128 * channel_multiplier,\n",
        "            256: 64 * channel_multiplier,\n",
        "            512: 32 * channel_multiplier,\n",
        "            1024: 16 * channel_multiplier,\n",
        "        }\n",
        "\n",
        "        convs = [ConvLayer(3, channels[size], 1)]\n",
        "\n",
        "        log_size = int(math.log(size, 2))\n",
        "\n",
        "        in_channel = channels[size]\n",
        "\n",
        "        for i in range(log_size, 2, -1):\n",
        "            out_channel = channels[2 ** (i - 1)]\n",
        "\n",
        "            convs.append(ResBlock(in_channel, out_channel, blur_kernel))\n",
        "\n",
        "            in_channel = out_channel\n",
        "\n",
        "        self.convs = nn.Sequential(*convs)\n",
        "\n",
        "        self.stddev_group = 4\n",
        "        self.stddev_feat = 1\n",
        "\n",
        "        self.final_conv = ConvLayer(in_channel + 1, channels[4], 3)\n",
        "        self.final_linear = nn.Sequential(\n",
        "            EqualLinear(channels[4] * 4 * 4, channels[4], activation=\"fused_lrelu\"),\n",
        "            EqualLinear(channels[4], 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        out = self.convs(input)\n",
        "\n",
        "        batch, channel, height, width = out.shape\n",
        "        group = min(batch, self.stddev_group)\n",
        "        stddev = out.view(\n",
        "            group, -1, self.stddev_feat, channel // self.stddev_feat, height, width\n",
        "        )\n",
        "        stddev = torch.sqrt(stddev.var(0, unbiased=False) + 1e-8)\n",
        "        stddev = stddev.mean([2, 3, 4], keepdims=True).squeeze(2)\n",
        "        stddev = stddev.repeat(group, 1, height, width)\n",
        "        out = torch.cat([out, stddev], 1)\n",
        "\n",
        "        out = self.final_conv(out)\n",
        "\n",
        "        out = out.view(batch, -1)\n",
        "        out = self.final_linear(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Setting"
      ],
      "metadata": {
        "id": "iBrjujgqda9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, autograd, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils import data\n",
        "import torch.distributed as dist\n",
        "from torchvision import transforms, utils\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    import wandb\n",
        "\n",
        "except ImportError:\n",
        "    wandb = None\n",
        "\n",
        "\n",
        "def data_sampler(dataset, shuffle, distributed):\n",
        "    if distributed:\n",
        "        return data.distributed.DistributedSampler(dataset, shuffle=shuffle)\n",
        "\n",
        "    if shuffle:\n",
        "        return data.RandomSampler(dataset)\n",
        "\n",
        "    else:\n",
        "        return data.SequentialSampler(dataset)\n",
        "\n",
        "\n",
        "def requires_grad(model, flag=True):\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = flag\n",
        "\n",
        "\n",
        "def accumulate(model1, model2, decay=0.999):\n",
        "    par1 = dict(model1.named_parameters())\n",
        "    par2 = dict(model2.named_parameters())\n",
        "\n",
        "    for k in par1.keys():\n",
        "        par1[k].data.mul_(decay).add_(par2[k].data, alpha=1 - decay)\n",
        "\n",
        "\n",
        "def sample_data(loader):\n",
        "    while True:\n",
        "        for batch in loader:\n",
        "            yield batch\n",
        "\n",
        "\n",
        "def d_logistic_loss(real_pred, fake_pred):\n",
        "    real_loss = F.softplus(-real_pred)\n",
        "    fake_loss = F.softplus(fake_pred)\n",
        "\n",
        "    return real_loss.mean() + fake_loss.mean()\n",
        "\n",
        "\n",
        "def d_r1_loss(real_pred, real_img):\n",
        "    with conv2d_gradfix.no_weight_gradients():\n",
        "        grad_real, = autograd.grad(\n",
        "            outputs=real_pred.sum(), inputs=real_img, create_graph=True\n",
        "        )\n",
        "    grad_penalty = grad_real.pow(2).reshape(grad_real.shape[0], -1).sum(1).mean()\n",
        "\n",
        "    return grad_penalty\n",
        "\n",
        "\n",
        "def g_nonsaturating_loss(fake_pred):\n",
        "    loss = F.softplus(-fake_pred).mean()\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def g_path_regularize(fake_img, latents, mean_path_length, decay=0.01):\n",
        "    noise = torch.randn_like(fake_img) / math.sqrt(\n",
        "        fake_img.shape[2] * fake_img.shape[3]\n",
        "    )\n",
        "    grad, = autograd.grad(\n",
        "        outputs=(fake_img * noise).sum(), inputs=latents, create_graph=True\n",
        "    )\n",
        "    path_lengths = torch.sqrt(grad.pow(2).sum(2).mean(1))\n",
        "\n",
        "    path_mean = mean_path_length + decay * (path_lengths.mean() - mean_path_length)\n",
        "\n",
        "    path_penalty = (path_lengths - path_mean).pow(2).mean()\n",
        "\n",
        "    return path_penalty, path_mean.detach(), path_lengths\n",
        "\n",
        "\n",
        "def make_noise(batch, latent_dim, n_noise, device):\n",
        "    if n_noise == 1:\n",
        "        return torch.randn(batch, latent_dim, device=device)\n",
        "\n",
        "    noises = torch.randn(n_noise, batch, latent_dim, device=device).unbind(0)\n",
        "\n",
        "    return noises\n",
        "\n",
        "\n",
        "def mixing_noise(batch, latent_dim, prob, device):\n",
        "    if prob > 0 and random.random() < prob:\n",
        "        return make_noise(batch, latent_dim, 2, device)\n",
        "\n",
        "    else:\n",
        "        return [make_noise(batch, latent_dim, 1, device)]\n",
        "\n",
        "\n",
        "def set_grad_none(model, targets):\n",
        "    for n, p in model.named_parameters():\n",
        "        if n in targets:\n",
        "            p.grad = None\n",
        "\n",
        "\n",
        "def train(args, loader, generator, discriminator, g_optim, d_optim, g_ema, device):\n",
        "    os.makedirs(f\"/content/out/{args.name}_sample\", exist_ok=True)\n",
        "    os.makedirs(f\"/content/out/{args.name}_checkpoint\", exist_ok=True)\n",
        "    loader = sample_data(loader)\n",
        "\n",
        "    pbar = range(args.iter)\n",
        "\n",
        "    if get_rank() == 0:\n",
        "        pbar = tqdm(pbar, initial=args.start_iter, dynamic_ncols=True, smoothing=0.01)\n",
        "\n",
        "    mean_path_length = 0\n",
        "\n",
        "    d_loss_val = 0\n",
        "    r1_loss = torch.tensor(0.0, device=device)\n",
        "    g_loss_val = 0\n",
        "    path_loss = torch.tensor(0.0, device=device)\n",
        "    path_lengths = torch.tensor(0.0, device=device)\n",
        "    mean_path_length_avg = 0\n",
        "    loss_dict = {}\n",
        "\n",
        "    if args.distributed:\n",
        "        g_module = generator.module\n",
        "        d_module = discriminator.module\n",
        "\n",
        "    else:\n",
        "        g_module = generator\n",
        "        d_module = discriminator\n",
        "\n",
        "    accum = 0.5 ** (32 / (10 * 1000))\n",
        "    ada_aug_p = args.augment_p if args.augment_p > 0 else 0.0\n",
        "    r_t_stat = 0\n",
        "\n",
        "    if args.augment and args.augment_p == 0:\n",
        "        ada_augment = AdaptiveAugment(args.ada_target, args.ada_length, 8, device)\n",
        "\n",
        "    sample_z = torch.randn(args.n_sample, args.latent, device=device)\n",
        "\n",
        "    for idx in pbar:\n",
        "        i = idx + args.start_iter\n",
        "\n",
        "        if i > args.iter:\n",
        "            print(\"Done!\")\n",
        "\n",
        "            break\n",
        "\n",
        "        real_img = next(loader)\n",
        "        real_img = real_img.to(device)\n",
        "\n",
        "        requires_grad(generator, False)\n",
        "        requires_grad(discriminator, True)\n",
        "\n",
        "        noise = mixing_noise(args.batch, args.latent, args.mixing, device)\n",
        "        fake_img, _ = generator(noise)\n",
        "\n",
        "        if args.augment:\n",
        "            real_img_aug, _ = augment(real_img, ada_aug_p)\n",
        "            fake_img, _ = augment(fake_img, ada_aug_p)\n",
        "\n",
        "        else:\n",
        "            real_img_aug = real_img\n",
        "\n",
        "        fake_pred = discriminator(fake_img)\n",
        "        real_pred = discriminator(real_img_aug)\n",
        "        d_loss = d_logistic_loss(real_pred, fake_pred)\n",
        "\n",
        "        loss_dict[\"d\"] = d_loss\n",
        "        loss_dict[\"real_score\"] = real_pred.mean()\n",
        "        loss_dict[\"fake_score\"] = fake_pred.mean()\n",
        "\n",
        "        discriminator.zero_grad()\n",
        "        d_loss.backward()\n",
        "        d_optim.step()\n",
        "\n",
        "        if args.augment and args.augment_p == 0:\n",
        "            ada_aug_p = ada_augment.tune(real_pred)\n",
        "            r_t_stat = ada_augment.r_t_stat\n",
        "\n",
        "        d_regularize = i % args.d_reg_every == 0\n",
        "\n",
        "        if d_regularize:\n",
        "            real_img.requires_grad = True\n",
        "\n",
        "            if args.augment:\n",
        "                real_img_aug, _ = augment(real_img, ada_aug_p)\n",
        "\n",
        "            else:\n",
        "                real_img_aug = real_img\n",
        "\n",
        "            real_pred = discriminator(real_img_aug)\n",
        "            r1_loss = d_r1_loss(real_pred, real_img)\n",
        "\n",
        "            discriminator.zero_grad()\n",
        "            (args.r1 / 2 * r1_loss * args.d_reg_every + 0 * real_pred[0]).backward()\n",
        "\n",
        "            d_optim.step()\n",
        "\n",
        "        loss_dict[\"r1\"] = r1_loss\n",
        "\n",
        "        requires_grad(generator, True)\n",
        "        requires_grad(discriminator, False)\n",
        "\n",
        "        noise = mixing_noise(args.batch, args.latent, args.mixing, device)\n",
        "        fake_img, _ = generator(noise)\n",
        "\n",
        "        if args.augment:\n",
        "            fake_img, _ = augment(fake_img, ada_aug_p)\n",
        "\n",
        "        fake_pred = discriminator(fake_img)\n",
        "        g_loss = g_nonsaturating_loss(fake_pred)\n",
        "\n",
        "        loss_dict[\"g\"] = g_loss\n",
        "\n",
        "        generator.zero_grad()\n",
        "        g_loss.backward()\n",
        "        g_optim.step()\n",
        "\n",
        "        g_regularize = i % args.g_reg_every == 0\n",
        "\n",
        "        if g_regularize:\n",
        "            path_batch_size = max(1, args.batch // args.path_batch_shrink)\n",
        "            noise = mixing_noise(path_batch_size, args.latent, args.mixing, device)\n",
        "            fake_img, latents = generator(noise, return_latents=True)\n",
        "\n",
        "            path_loss, mean_path_length, path_lengths = g_path_regularize(\n",
        "                fake_img, latents, mean_path_length\n",
        "            )\n",
        "\n",
        "            generator.zero_grad()\n",
        "            weighted_path_loss = args.path_regularize * args.g_reg_every * path_loss\n",
        "\n",
        "            if args.path_batch_shrink:\n",
        "                weighted_path_loss += 0 * fake_img[0, 0, 0, 0]\n",
        "\n",
        "            weighted_path_loss.backward()\n",
        "\n",
        "            g_optim.step()\n",
        "\n",
        "            mean_path_length_avg = (\n",
        "                reduce_sum(mean_path_length).item() / get_world_size()\n",
        "            )\n",
        "\n",
        "        loss_dict[\"path\"] = path_loss\n",
        "        loss_dict[\"path_length\"] = path_lengths.mean()\n",
        "\n",
        "        accumulate(g_ema, g_module, accum)\n",
        "\n",
        "        loss_reduced = reduce_loss_dict(loss_dict)\n",
        "\n",
        "        d_loss_val = loss_reduced[\"d\"].mean().item()\n",
        "        g_loss_val = loss_reduced[\"g\"].mean().item()\n",
        "        r1_val = loss_reduced[\"r1\"].mean().item()\n",
        "        path_loss_val = loss_reduced[\"path\"].mean().item()\n",
        "        real_score_val = loss_reduced[\"real_score\"].mean().item()\n",
        "        fake_score_val = loss_reduced[\"fake_score\"].mean().item()\n",
        "        path_length_val = loss_reduced[\"path_length\"].mean().item()\n",
        "\n",
        "        if get_rank() == 0:\n",
        "            pbar.set_description(\n",
        "                (\n",
        "                    f\"d: {d_loss_val:.4f}; g: {g_loss_val:.4f}; r1: {r1_val:.4f}; \"\n",
        "                    f\"path: {path_loss_val:.4f}; mean path: {mean_path_length_avg:.4f}; \"\n",
        "                    f\"augment: {ada_aug_p:.4f}\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "            if wandb and args.wandb:\n",
        "                wandb.log(\n",
        "                    {\n",
        "                        \"Generator\": g_loss_val,\n",
        "                        \"Discriminator\": d_loss_val,\n",
        "                        \"Augment\": ada_aug_p,\n",
        "                        \"Rt\": r_t_stat,\n",
        "                        \"R1\": r1_val,\n",
        "                        \"Path Length Regularization\": path_loss_val,\n",
        "                        \"Mean Path Length\": mean_path_length,\n",
        "                        \"Real Score\": real_score_val,\n",
        "                        \"Fake Score\": fake_score_val,\n",
        "                        \"Path Length\": path_length_val,\n",
        "                    }\n",
        "                )\n",
        "\n",
        "            if i % 50 == 0: #in this code, interval is 50 for Convenience but for real, i did it with 10\n",
        "                with torch.no_grad():\n",
        "                    g_ema.eval()\n",
        "                    sample, _ = g_ema([sample_z])\n",
        "                    utils.save_image(\n",
        "                        sample,\n",
        "                        f\"/content/out/{args.name}_sample/{str(i).zfill(6)}.png\",\n",
        "                        nrow=int(args.n_sample ** 0.5),\n",
        "                        normalize=True,\n",
        "                        range=(-1, 1),\n",
        "                    )\n",
        "\n",
        "            if i % 50 == 0:\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"g\": g_module.state_dict(),\n",
        "                        \"d\": d_module.state_dict(),\n",
        "                        \"g_ema\": g_ema.state_dict(),\n",
        "                        \"g_optim\": g_optim.state_dict(),\n",
        "                        \"d_optim\": d_optim.state_dict(),\n",
        "                        \"args\": args,\n",
        "                        \"ada_aug_p\": ada_aug_p,\n",
        "                    },\n",
        "                    f\"/content/out/{args.name}_checkpoint/{str(i).zfill(6)}.pt\",\n",
        "                )"
      ],
      "metadata": {
        "id": "CqjFyusyc20M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TrainingArgs"
      ],
      "metadata": {
        "id": "omQbrznAK-vP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingArgs(object):\n",
        "  def __init__(self,mixing,ada_target,ada_length,batch,ada_every,ckpt,lr,name):\n",
        "    self.device = \"cuda\"\n",
        "    self.path = \"/content/data\"\n",
        "    self.arch = \"stylegan2\"\n",
        "    self.iter = 1001\n",
        "    self.batch = 1\n",
        "    self.n_sample = 25\n",
        "    self.size = 64\n",
        "    self.r1 = 10\n",
        "    self.path_regularize = 2\n",
        "    self.path_batch_shrink = 2\n",
        "    self.d_reg_every = 2\n",
        "    self.g_reg_every = 4\n",
        "    self.mixing = mixing\n",
        "    self.ckpt = ckpt\n",
        "    self.lr = lr\n",
        "    self.channel_multiplier = 2\n",
        "    self.wandb = False\n",
        "    self.local_rank = 0\n",
        "    self.augment = True\n",
        "    self.augment_p = 0\n",
        "    self.ada_target = ada_target\n",
        "    self.ada_length = ada_length\n",
        "    self.ada_every = 4\n",
        "    self.distributed = False\n",
        "    self.latent = 512\n",
        "    self.n_mlp = 8\n",
        "    self.start_iter = 0\n",
        "    self.seed = 2\n",
        "    self.name = name"
      ],
      "metadata": {
        "id": "4uwyIYD6eNPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "SFGKE1uweHWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mixing = 0.7\n",
        "ada_target = 0.6\n",
        "ada_length = 1000\n",
        "batch = 1\n",
        "ada_every = 64\n",
        "lr = 0.0001\n",
        "ckpt = \"/content/out/mixing_0.9_412/checkpoint/000200.pt\" #Transfer from this checkpoint\n",
        "originalckpt = \"/content/celebahq_100000.pt\" #This is for discriminator\n",
        "args = TrainingArgs(mixing,\n",
        "                    ada_target,\n",
        "                    ada_length,\n",
        "                    batch,\n",
        "                    ada_every,\n",
        "                    ckpt,\n",
        "                    lr,\n",
        "                    name = f\"transfer_tiral2_adam,lr:{lr}_at:{ada_target}_al:{ada_length}_m:{mixing}_b:{batch}_ae:{ada_every}\")\n",
        "\n",
        "generator = Generator(\n",
        "    args.size, args.latent, args.n_mlp, channel_multiplier=args.channel_multiplier\n",
        ").to(args.device)\n",
        "discriminator = Discriminator(\n",
        "    args.size, channel_multiplier=args.channel_multiplier\n",
        ").to(args.device)\n",
        "g_ema = Generator(\n",
        "    args.size, args.latent, args.n_mlp, channel_multiplier=args.channel_multiplier\n",
        ").to(args.device)\n",
        "g_ema.eval()\n",
        "accumulate(g_ema, generator, 0)\n",
        "\n",
        "g_reg_ratio = args.g_reg_every / (args.g_reg_every + 1)\n",
        "d_reg_ratio = args.d_reg_every / (args.d_reg_every + 1)\n",
        "\n",
        "g_optim = optim.AdamW(\n",
        "    generator.parameters(),\n",
        "    lr=args.lr * g_reg_ratio,\n",
        "    betas=(0 ** g_reg_ratio, 0.99 ** g_reg_ratio),\n",
        ")\n",
        "d_optim = optim.AdamW(\n",
        "    discriminator.parameters(),\n",
        "    lr=args.lr * d_reg_ratio,\n",
        "    betas=(0 ** d_reg_ratio, 0.99 ** d_reg_ratio),\n",
        ")\n",
        "\n",
        "if args.ckpt is not None:\n",
        "    print(\"load model:\", args.ckpt)\n",
        "\n",
        "    ckpt = torch.load(args.ckpt, map_location=lambda storage, loc: storage)\n",
        "    original_ckpt = torch.load(originalckpt, map_location=lambda storage, loc: storage)\n",
        "    try:\n",
        "        ckpt_name = os.path.basename(args.ckpt)\n",
        "        args.start_iter = int(os.path.splitext(ckpt_name)[0])\n",
        "\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "    generator.load_state_dict(ckpt[\"g_ema\"]) #generator weight from NADA-trained\n",
        "    discriminator.load_state_dict(original_ckpt[\"d\"]) #discriminator weight from original ckpt\n",
        "    g_ema.load_state_dict(ckpt[\"g_ema\"])\n",
        "\n",
        "    g_optim.load_state_dict(ckpt[\"g_optim\"])\n",
        "    d_optim.load_state_dict(original_ckpt[\"d_optim\"])\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True),\n",
        "    ]\n",
        ")\n",
        "\n",
        "dataset = ImageDataset(args.path, transform)\n",
        "loader = data.DataLoader(\n",
        "    dataset,batch_size=args.batch)\n",
        "\n",
        "#Set Seed\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "random.seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "\n",
        "train(args, loader, generator, discriminator, g_optim, d_optim, g_ema, args.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpApXI45d64_",
        "outputId": "6da62153-1b3f-4302-daf4-88cce76dcdf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load model: /content/out/mixing_0.9_412/checkpoint/000200.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "d: 0.4496; g: 2.0100; r1: 0.0140; path: 0.0002; mean path: 0.0829; augment: 0.0320: : 1002it [04:24,  3.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate and Evaluate"
      ],
      "metadata": {
        "id": "JCp9gmxwg_E6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " you can find sample images from\n",
        " \"/conten/out/transfer_tiral2_adam,lr:0.0001_at:0.6_al:1000_m:0. 7_b:1_ae:64_sample\"  \n",
        " and final result image from /content/fake_images\n"
      ],
      "metadata": {
        "id": "d0wCuQLyRgd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from model.sg2_model import Generator\n",
        "import torch\n",
        "import os\n",
        "\n",
        "from torchvision import utils\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generate(model, result_path, n_samples, args):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        os.makedirs(result_path, exist_ok=True)\n",
        "        for i in tqdm(range(n_samples), desc='Generating...'):\n",
        "            z = torch.randn(1, 512).cuda()\n",
        "            fake_x, _ = model([z], truncation=args.truncation, truncation_latent=None)\n",
        "            utils.save_image(fake_x, os.path.join(result_path, '{}.png'.format(i)), normalize=True, value_range=(-1, 1))\n",
        "\n",
        "class InferenceArgs(object):\n",
        "    def __init__(self,ckpt):\n",
        "        self.ckpt = ckpt\n",
        "        self.result_path = \"/content/fake_images\"\n",
        "        self.n_samples = 5000\n",
        "        self.truncation = 1\n",
        "        self.truncation_mean = 4096\n",
        "        self.size = 64\n",
        "        self.latent = 512\n",
        "        self.n_mlp = 8\n",
        "        self.channel_multiplier = 2\n",
        "        self.seed = 0"
      ],
      "metadata": {
        "id": "A1Mafl7OsrEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate Image\n",
        "file_paths = [] #ckpt 파일들에 대한 array 생성\n",
        "directory_path = f\"/content/out/transfer_tiral2_adam,lr:0.0001_at:0.6_al:1000_m:0.7_b:1_ae:64_checkpoint\"\n",
        "for root, directories, files in os.walk(directory_path):\n",
        "    for filename in files:\n",
        "        # 파일 경로를 생성하고 리스트에 추가합니다.\n",
        "        file_path = os.path.join(root, filename)\n",
        "        file_paths.append(file_path)\n",
        "\n",
        "file_paths = sorted(file_paths) #순서대로 정렬\n",
        "os.makedirs(f\"/content/drive/MyDrive/khuggle_submission/submit/{args.name}\", exist_ok=True)#저장할 폴더 생성\n",
        "#make submissionfile\n",
        "for i,path in enumerate(file_paths): #각각의 체크포인트에 대한 submission 파일 생성\n",
        "  if i >= 6 : #500 iteration 부터 evalutation (경험적으로 여기서부터 성능이 좋았음)\n",
        "    args_inf = InferenceArgs(ckpt = path)\n",
        "    args_inf.seed = 0\n",
        "\n",
        "    #Setting Seed\n",
        "    torch.manual_seed(args_inf.seed)\n",
        "    torch.cuda.manual_seed(args_inf.seed)\n",
        "    random.seed(args_inf.seed)\n",
        "    np.random.seed(args_inf.seed)\n",
        "\n",
        "    ckpt = torch.load(args_inf.ckpt)\n",
        "\n",
        "    model = Generator(args_inf.size, args_inf.latent, args_inf.n_mlp).cuda()\n",
        "\n",
        "    model.load_state_dict(ckpt[\"g_ema\"], strict=False)\n",
        "\n",
        "    generate(model, args_inf.result_path, args_inf.n_samples, args_inf)\n",
        "    #extract Inception Feature\n",
        "    !python /content/extract_inception_feature.py --fake_path /content/fake_images --output inception_feat.npy\n",
        "    #Compute LPIPS Matrix\n",
        "    !python /content/compute_lpips_matrix.py --real_path /content/data --fake_path /content/fake_images --output lpips_matrix.npy\n",
        "    inception_feat = np.load('inception_feat.npy')\n",
        "    lpips_matrix = np.load('lpips_matrix.npy', allow_pickle=True).item()\n",
        "    # 값 저장 (콜랍 백엔드에 저장하면 많이 다운로드가 느려 구글드라이브로 다운로드)\n",
        "    np.save(f\"/content/drive/MyDrive/khuggle_submission/submit/{args.name}/seed:{args_inf.seed}_{args.name}_e:{path[-7:-3]}.npy\", {\"inception_feat\": inception_feat, \"entropy\": lpips_matrix[\"entropy\"], \"dists\": lpips_matrix[\"dists\"]})\n",
        "    torch.save(ckpt,f\"/content/drive/MyDrive/khuggle_submission/submit/{args.name}/seed:{args_inf.seed}_e:{path[-7:-3]}.pt\")"
      ],
      "metadata": {
        "id": "hbRRG-hg2Qo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My_Own Pretrained Model"
      ],
      "metadata": {
        "id": "51nQgvvBB6WZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-bGxWPRSPpVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content"
      ],
      "metadata": {
        "id": "u2EMQF35R9lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://drive.google.com/uc?id=13mlMmU8Yacwss4nOLYUjFTfXBuzjyKnc\"\n",
        "!gdown {url}"
      ],
      "metadata": {
        "id": "EKIqlbKXSy8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make Submission of my Model"
      ],
      "metadata": {
        "id": "AKqAE9lmQb-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args_inf = InferenceArgs(ckpt = \"/content/seed:0_e0570.pt\")\n",
        "args_inf.seed = 0\n",
        "\n",
        "#Setting Seed\n",
        "torch.manual_seed(args_inf.seed)\n",
        "torch.cuda.manual_seed(args_inf.seed)\n",
        "random.seed(args_inf.seed)\n",
        "np.random.seed(args_inf.seed)\n",
        "\n",
        "ckpt = torch.load(args_inf.ckpt)\n",
        "\n",
        "model = Generator(args_inf.size, args_inf.latent, args_inf.n_mlp).cuda()\n",
        "#images is generated in fake_images directory\n",
        "\n",
        "model.load_state_dict(ckpt[\"g_ema\"], strict=False)\n",
        "\n",
        "generate(model, args_inf.result_path, args_inf.n_samples, args_inf)\n",
        "#extract Inception Feature\n",
        "!python /content/extract_inception_feature.py --fake_path /content/fake_images --output inception_feat.npy\n",
        "#Compute LPIPS Matrix\n",
        "!python /content/compute_lpips_matrix.py --real_path /content/data --fake_path /content/fake_images --output lpips_matrix.npy\n",
        "inception_feat = np.load('inception_feat.npy')\n",
        "lpips_matrix = np.load('lpips_matrix.npy', allow_pickle=True).item()\n",
        "#save value\n",
        "np.save(\"my_result.npy\", {\"inception_feat\": inception_feat, \"entropy\": lpips_matrix[\"entropy\"], \"dists\": lpips_matrix[\"dists\"]})\n"
      ],
      "metadata": {
        "id": "3sg1Pkm2QnaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "taggtVQHTwQa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}